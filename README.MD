Loading and preparing data...
Loaded 53310 conversations
Vocabulary size: 973
Encoder vocabulary saved to encoder_vocab.pt
Train data: 7,226,814 tokens
Val data: 1,389,803 tokens

Initializing model...
Model has 11,487,181 parameters

Starting training for 5000 iterations...
Block size: 256, Batch size: 32, LR: 0.0003
------------------------------------------------------------
Step     0: train loss 7.0958, val loss 7.0968
Step   500: train loss 1.9642, val loss 1.9877
Step  1000: train loss 1.6185, val loss 1.6388
Step  1500: train loss 1.3795, val loss 1.4130
Step  2000: train loss 1.1327, val loss 1.1895
Step  2500: train loss 1.0491, val loss 1.1129
Step  3000: train loss 1.0076, val loss 1.0742
Step  3500: train loss 0.9803, val loss 1.0511
Step  4000: train loss 0.9596, val loss 1.0414
Step  4500: train loss 0.9331, val loss 1.0209
Step  4999: train loss 0.9149, val loss 1.0079